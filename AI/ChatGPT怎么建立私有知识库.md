# 方案1. ChatGPT Fine-tune自己的模型

ChatGPT的话，当前能做到的唯一方式就是官方说的微调出一个自己的模型，然后上传到ChatGPT，聊天时选用这个模型就好了。做法也不复杂，主要就是提供官方指定格式的数据集就可以了

# 方案2. Azure AI Studio云端训练自己的GPT

如果你想私有部署GPT的话，你估计要等一等，很快也可以做到了。因为微软刚刚开发者大会展示了它正在做这个事情。你可以看下我分享出来的这个视频，其中就有描述他们正在做Azure AI Studio怎么让用户用自己的私有数据在自己的云空间训练自己的GPT

# 方案3. ChatGPT + pinecone向量数据库和你私有数据对话

通过ChatGPT + pinecone向量数据库来做，将你的私有知识都存在向量数据库中，然后对你数据做语义查询。大家都知道chatgpt预训练完成后，会生成一个embeddings向量字典，我们可以将这个利用起来。比如我们可以将我们的私有知识库各个章节通过openai的相关api获取到对应的embeddings，然后将这些embeddings保存到pinecone向量数据库，当用户要对某个领域后者问题进行语义查询时，则将用户的输入同样通过openai的相关api来获取相应的embeddings向量，然后再和向量数据库pinecone中的我们的私有知识库类型做语义相似度查询，然后返回给用户。

当然，实现期间你很可能需要用到一个叫做LangChain的库，其实这是个很火的大语言模型开发框架，我之前也有分享过相关的知识

# 方案4. 使用开源LLM本地部署和微调

最后，如果你可以考虑非ChatGPT，即一些开源的LLM的话，其实可选择的方案就非常的多了。因为现在已经有很多开源的LLM可以让你在本地跑，根本不需要网络。比如羊驼家族的Vicuna，Alpaca这些，你只要在人家的基础上进行微调就可以了。

# 方案5. PrivateGPT

这是前段时间分享的另外一个方法，其实应该算是方案3加方案4的一个具体实现方案，但胜在可操作性强，根据视频一步步走就是了，完了大家可以在此基础上做自己的增强。这里一并分享给大家

# 方案6. Quivr

这是一个很流行的方案3的落地实现方案，我看其他答主也已经有提供相应的文字答案，大家可以看下。




# 目前开源模型回答还不够稳定

| 模型             | ChatGLM-6B                                                                               | ChatGLM2-6B                                                                                                        | Vicuna-13B                                                                                                                  | baichuan-7b                                                                                                       |
| ---------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| 占用空间(G)      | 24                                                                                       | 24                                                                                                                 | 50                                                                                                                          | 27                                                                                                                |
| 支持中文         | 是                                                                                       | 是                                                                                                                 | 是                                                                                                                          | 是                                                                                                                |
| 支持上下文token  | 2k                                                                                       | 8k                                                                                                                 | 2k                                                                                                                          | 4k                                                                                                                |
| 推理速度(字符/s) | 31.49                                                                                    | 44.62                                                                                                              |                                                                                                                             |                                                                                                                   |
| 问答效果         | 优点：1.推理速度快2.用户可以在消费级的显卡上进行本地部署缺点：1.会胡说八道2.知识总结啰嗦 | 优点：1.推理速度快2.能接收更大的上下文3.知识总结不啰嗦4.用户可以在消费级的显卡上进行本地部署缺点：复杂问答效果不好 | 优点：1.复杂任务比如text2sql, text2json表现还不错缺点：1、推理速度慢2.总结性回答比较啰嗦3.上下文token支持太小，容易出现乱码 | 优点：1.推理速度比chatglm稍微慢一点2.能接收更大的上下文3.知识总结不啰嗦缺点：1.复杂问答效果不好2.受prompt影响较大 |
