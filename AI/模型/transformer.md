- [向量间的距离 计算](#向量间的距离-计算)
- [每个单词的初始嵌入（embedding）](#每个单词的初始嵌入embedding)
- [语义信息和语境信息](#语义信息和语境信息)
- [向量的点积](#向量的点积)
- [单位矩阵](#单位矩阵)
- [梯度](#梯度)
- [损失函数](#损失函数)
- [残差连接](#残差连接)
- [前馈神经网络](#前馈神经网络)
- [偏置](#偏置)
- [层归一化](#层归一化)
- [梯度爆炸](#梯度爆炸)
- [Attention\_Is\_All\_You\_Need 翻译版](#attention_is_all_you_need-翻译版)
- [Softmax函数](#softmax函数)
- [全连接层的权重W 和 偏置b](#全连接层的权重w-和-偏置b)
- [attention模型](#attention模型)
- [Soft-Attention](#soft-attention)
- [--](#--)

# 向量间的距离 计算
计算向量之间的距离通常使用欧几里得距离（Euclidean distance）或曼哈顿距离（Manhattan distance）。下面是这两种距离计算的示例：

1. 欧几里得距离：
欧几里得距离是计算两个向量之间的直线距离。如果有两个向量A和B，每个向量有n个元素（a1, a2, ..., an和b1, b2, ..., bn），则欧几里得距离可以通过以下公式计算：
```
distance = sqrt((a1 - b1)^2 + (a2 - b2)^2 + ... + (an - bn)^2)
```
这里的sqrt表示平方根。

2. 曼哈顿距离：
曼哈顿距离是计算两个向量之间的曼哈顿街区距离，也就是沿着网格线的距离总和。如果有两个向量A和B，每个向量有n个元素（a1, a2, ..., an和b1, b2, ..., bn），则曼哈顿距离可以通过以下公式计算：
```
distance = |a1 - b1| + |a2 - b2| + ... + |an - bn|
```
这里的| |表示绝对值。

这些是两种常见的向量距离计算方法，你可以根据具体情况选择适合的距离度量方法。

# 每个单词的初始嵌入（embedding）
在深度学习的自然语言处理（NLP）中，"嵌入"或"词嵌入"（Word Embedding）是一种将词语或者词汇项映射到向量的技术。这个向量捕捉了词的语义信息和语境信息。

为什么需要词嵌入？在计算机中，我们需要找到一种方式来表示词语，让计算机可以理解和处理。最简单的方式可能就是独热编码（One-Hot Encoding），即对于词汇表中的每个词，我们创建一个全为0的向量，只有对应该词的位置为1。但是这种方法有两个主要问题：1）向量的长度等于词汇表的大小，当词汇表非常大时，这种表示非常稀疏和低效；2）它不能捕捉词语间的语义关系，所有词的向量都是正交的，彼此之间的距离都是相同的。

词嵌入通过学习一种密集的向量表示，可以解决这两个问题。在词嵌入中，每个词都被表示为一个实数向量，通常维度会远小于词汇表的大小（例如，几百到几千）。并且，这些向量是通过在大量的文本数据上训练得到的，因此它们可以捕捉到词语间的语义关系。例如，相似的词应该有相似的向量，我们可以通过计算向量间的距离来衡量词语的相似性。

"初始嵌入"通常是指在开始训练深度学习模型之前，为每个词分配的词嵌入。这些初始嵌入可以是随机的，也可以是预先训练好的，例如使用Word2Vec或GloVe算法得到的词嵌入。在训练过程中，这些嵌入会被进一步调整，以便更好地完成特定的任务。

# 语义信息和语境信息
语义信息和语境信息是自然语言处理中两个重要的概念。

1. 语义信息：
语义信息指的是文本或语言中的词汇、短语、句子或段落所携带的意义或含义。它关注的是语言表达的基本语义内容，独立于具体的语言使用环境。语义信息可以通过词汇、语法和语义结构来表达。例如，在句子"I have a pet dog"中，语义信息包括"我"、"有"、"宠物"和"狗"等词汇的意义，以及它们之间的关系。

2. 语境信息：
语境信息指的是文本或语言中的词汇、短语、句子或段落在特定上下文中所具有的意义或含义。它强调语言表达的依赖于特定的语言使用环境和背景知识。语境信息考虑了说话者和听话者之间的共享知识、文化背景、上下文环境等因素。语境信息可以通过上下文中的指代、暗示、推理等方式来表达。例如，在对话中的句子"I'm freezing"，它的语境信息可能是说话者感到非常冷，而非字面上的结冰。

在自然语言处理任务中，理解和处理语义信息和语境信息对于正确解析和生成自然语言文本至关重要。语义信息有助于词义消歧、实体识别、句法分析等任务，而语境信息则有助于指代消解、语义角色标注、情感分析等任务。同时，语义信息和语境信息的结合能够提高自然语言处理系统在实际应用中的性能和准确性。

# 向量的点积
给定两个n维向量A和B，它们可以表示为：
A = [a1, a2, ..., an]
B = [b1, b2, ..., bn]

向量的点积可以通过将对应位置的元素相乘，并将乘积求和来计算，即：
A·B = a1 * b1 + a2 * b2 + ... + an * bn

其中，·表示点积运算符。

点积具有以下几个重要的性质：

    交换律：A·B = B·A
    分配律：(A + B)·C = A·C + B·C
    数量乘法：(kA)·B = k(A·B)

# 单位矩阵
单位矩阵，也称为恒等矩阵或单位阵，是一个特殊的方阵，其中对角线上的元素均为1，而其它位置上的元素均为0。单位矩阵通常用符号I表示。具体而言，对于一个n×n的单位矩阵，其定义如下：
```
I = [[1, 0, 0, ..., 0],
     [0, 1, 0, ..., 0],
     [0, 0, 1, ..., 0],
     ...
     [0, 0, 0, ..., 1]]
```
其中，对角线上的元素为1，而其它位置上的元素都是0。

单位矩阵在线性代数和矩阵运算中起着重要的作用。它具有以下几个特性：

1. 单位矩阵是方阵：单位矩阵的行数等于列数，因此它是一个方阵。

2. 单位矩阵的主对角线上的元素都是1：单位矩阵的主对角线是从左上角到右下角的对角线，该对角线上的元素均为1。

3. 单位矩阵与矩阵的乘法：单位矩阵与任何相同尺寸的矩阵相乘，结果都是该矩阵本身。即对于任意n×m的矩阵A，有IA = AI = A，其中I是适当尺寸的单位矩阵。

4. 单位矩阵的逆矩阵：单位矩阵的逆矩阵等于它本身。即对于n×n的单位矩阵I，有I⁻ = I。

# 梯度
在机器学习中，梯度通常指的是目标函数（损失函数）对于模型参数的偏导数

# 损失函数
损失函数在机器学习和深度学习中扮演着重要的角色，用于衡量模型的预测结果与实际标签之间的差异程度。下面是两个常见的损失函数的例子：

1. 均方误差（Mean Squared Error, MSE）损失函数：
均方误差是用于回归问题的常见损失函数。它衡量模型预测值与真实标签之间的差异的平方和的平均值。

假设我们有一组样本数据，每个样本有一个实际标签y和模型的预测值y_hat。那么均方误差损失函数可以定义为：
```
MSE = (1/N) * Σ(y - y_hat)^2
```
其中，N是样本的总数，Σ表示求和运算。

举个例子，假设我们有以下四个样本的真实标签和模型预测值：
样本1：y = 5，y_hat = 4
样本2：y = 2，y_hat = 3
样本3：y = 7，y_hat = 6
样本4：y = 9，y_hat = 9

我们可以计算均方误差损失函数：
```
MSE = (1/4) * [(5-4)^2 + (2-3)^2 + (7-6)^2 + (9-9)^2]
    = (1/4) * [1 + 1 + 1 + 0]
    = 0.75
```
这个均方误差值表示模型的平均预测误差的平方。

2. 交叉熵（Cross Entropy）损失函数：
交叉熵是用于分类问题的常见损失函数。它衡量模型的输出概率分布与真实标签之间的差异。

假设我们有一组样本数据，每个样本属于某个类别，且有真实标签y和模型的输出概率分布y_hat。那么交叉熵损失函数可以定义为：
```
CrossEntropy = -Σ(y * log(y_hat))
```
其中，Σ表示求和运算，log表示自然对数。

举个例子，假设我们有以下三个样本的真实标签和模型的输出概率分布：
样本1：y = [0, 1, 0]，y_hat = [0.2, 0.7, 0.1]
样本2：y = [1, 0, 0]，y_hat = [0.6, 0.3, 0.1]
样本3：y = [0, 0, 1]，y_hat = [0.1, 0.2, 0.7]

我们可以计算交叉熵损失函数：
```
CrossEntropy = -(0 * log(0.2) + 1 * log(0.7) + 0 * log(0.1) + 1 * log(0.6) + 0 * log(0.3) + 0 * log(0.1) + 0 * log(0.1) + 0 * log(0.2) + 1 * log(0.7))
            = -(0 + log(0.7) + 0 + log(0.6) + 0 + 0 + 0 + 0 + log(0.7))
            = -(log(0.7) + log(0.6) + log(0.7))
            ≈ 1.693
```
这个交叉熵值表示模型的输出概率分布与真实标签之间的差异程度。

这些是常见的损失函数示例，用于衡量模型预测结果与真实标签之间的差异。在实际应用中，我们根据具体问题和任务的要求选择合适的损失函数来训练和优化模型。

# 残差连接
残差连接是一种在深度学习模型中常用的技术，可以帮助信息在网络层之间更好地传递。以下是一个例子来说明上述提到的残差连接。

假设我们有一个深度学习模型，包含两个子层：子层A和子层B。每个子层都有一个输入x和一个输出y。

在没有残差连接的情况下，子层A的输出将直接作为子层B的输入。这可以表示为：
```
y_B = Sublayer_B(y_A)
```

现在，我们引入残差连接来改进信息的传递。在残差连接中，我们将子层A的输入x添加到子层B的输出上，然后再应用层归一化。这可以表示为：
```
y_B = LayerNorm(y_A + Sublayer_B(y_A))
```

这样做的好处是，如果子层B能够提取出有用的特征，那么添加残差连接后，信息可以更直接地传递给子层B，而不会丢失太多原始输入的信息。

在这个例子中，残差连接的目的是加强信息流动，并帮助模型更好地学习输入和输出之间的映射关系。这种方法在许多深度学习模型中都得到了广泛应用，包括残差神经网络（ResNet）和变换器（Transformer）等。

# 前馈神经网络
前馈神经网络（Feed-forward Neural Network）是一种最基本的神经网络模型，也被称为多层感知机（Multi-Layer Perceptron, MLP）。它由输入层、若干个隐藏层和输出层组成，信息从输入层流向输出层，不会在网络中形成回路。

下面是一个简单的前馈神经网络的例子：

假设我们要构建一个用于数字分类的前馈神经网络，其中输入是一张28x28像素的手写数字图片（784个输入节点），输出是数字0到9的十个类别（10个输出节点）。

我们可以设计一个包含一个或多个隐藏层的前馈神经网络。例如，我们可以构建一个有两个隐藏层的前馈神经网络，每个隐藏层都有256个神经元。

这个网络的结构如下：

输入层（784个节点） -> 隐藏层1（256个节点） -> 隐藏层2（256个节点） -> 输出层（10个节点）

每个节点都与前一层的所有节点连接，并带有一个权重值。在每个节点处，输入通过权重进行加权求和，然后经过一个激活函数进行非线性转换。

例如，我们可以使用ReLU（Rectified Linear Unit）作为激活函数。ReLU函数定义为f(x) = max(0, x)，将负数变为零，保留正数不变。

在网络的前向传播过程中，输入数据通过层与层之间的连接传递。具体来说，输入数据从输入层传递到隐藏层1，再从隐藏层1传递到隐藏层2，最后从隐藏层2传递到输出层。每个层中的节点根据其输入和权重进行加权求和，然后通过激活函数进行非线性转换。

最后，在输出层，我们可以使用Softmax函数将输出转化为概率分布，表示输入属于每个类别的概率。

通过训练这个前馈神经网络，调整权重和偏置，使得网络能够对手写数字进行准确分类。

总结起来，前馈神经网络是一种信息只向前流动的神经网络模型，通过层与层之间的连接将输入数据传递到输出层。通过设计不同的网络结构和激活函数，前馈神经网络可以用于解决各种机器学习和深度学习任务，如图像分类、语音识别等。

# 偏置
在神经网络中，偏置（bias）是一种与权重（weight）类似的参数，用于调节神经元的激活阈值。它对于神经元的激活与输出起到重要作用。

举一个简单的线性回归模型的例子来介绍偏置的作用：

假设我们有一个简单的线性回归模型，其中有一个输入特征 x 和一个输出 y。我们希望通过模型预测输入特征 x 对应的输出 y。

线性回归模型的公式可以表示为：
```
y = wx + b
```
其中，w 是权重，x 是输入特征，b 是偏置。

权重 w 控制了输入特征 x 对输出 y 的影响程度，而偏置 b 则调整了整体的偏移量。

假设我们训练了一个线性回归模型，得到了权重 w=2 和偏置 b=1。那么，对于一个特定的输入 x=3，我们可以计算输出 y：
```
y = 2 * 3 + 1 = 7
```

在这个例子中，偏置 b 为 1，它使得模型在 x=0 时的输出为 1。通过调整偏置，我们可以在不改变权重的情况下，改变模型输出的整体偏移。

偏置在神经网络的每个神经元中都存在，并且对于不同的神经元可以有不同的偏置值。它可以增加模型的灵活性，允许神经元对输入的偏好有所不同。

总结起来，偏置是神经网络中的参数之一，用于调节神经元的激活阈值。它可以调整模型的整体偏移量，增加模型的灵活性，并在神经网络中起到重要的作用。

# 层归一化
层归一化（Layer Normalization）是一种用于神经网络中的归一化技术，它在每个神经网络层的输出上进行归一化操作。相比于批归一化（Batch Normalization），层归一化更适用于序列数据和小批量训练。

下面以一个示例来介绍层归一化的操作过程：

假设我们有一个包含4个神经元的全连接层，输出为一个4维的向量：[x1, x2, x3, x4]。对于该层的每个神经元，层归一化的操作如下：

1. 计算平均值和方差：首先，计算输出向量的平均值 μ 和方差 σ^2。

   μ = (x1 + x2 + x3 + x4) / 4
   
   σ^2 = [(x1 - μ)^2 + (x2 - μ)^2 + (x3 - μ)^2 + (x4 - μ)^2] / 4
   
2. 归一化：使用计算得到的平均值和方差对每个神经元的输出进行归一化。

   y1 = (x1 - μ) / √(σ^2 + ε)
   
   y2 = (x2 - μ) / √(σ^2 + ε)
   
   y3 = (x3 - μ) / √(σ^2 + ε)
   
   y4 = (x4 - μ) / √(σ^2 + ε)

   其中，ε是一个小的正数，用于防止方差为零的情况。

3. 缩放和平移：对归一化后的输出进行缩放和平移，以使其适应不同的数据分布。

   z1 = γ * y1 + β
   
   z2 = γ * y2 + β
   
   z3 = γ * y3 + β
   
   z4 = γ * y4 + β

   其中，γ和β是可学习的参数，用于缩放和平移归一化后的值。

通过层归一化操作，每个神经元的输出都被归一化到均值为0、方差为1的分布，并且可以通过学习参数 γ 和 β 进行进一步的缩放和平移操作。这有助于提高网络的稳定性和泛化能力，并且可以减少对学习率的依赖。层归一化通常应用于深度神经网络的每个层，以帮助网络更好地学习和适应不同的输入数据。

# 梯度爆炸
梯度爆炸（Gradient Explosion）是在深度学习中一种常见的问题，它指的是在反向传播过程中梯度值变得非常大，导致模型的权重更新变得不稳定，难以收敛。

让我们以一个简单的线性回归模型为例来说明梯度爆炸的问题。假设我们有一个包含一个输入特征x和一个目标值y的训练集。我们的模型可以表示为：

y = wx + b

其中w是权重，b是偏差。我们使用均方误差（Mean Square Error）作为损失函数，即：

L = (1/N) * Σ(y - (wx + b))^2

现在我们想要通过梯度下降来更新权重w和偏差b，以使损失函数最小化。梯度下降的步骤如下：

1. 随机初始化权重w和偏差b。
2. 前向传播：计算模型的预测值y_pred = wx + b。
3. 计算损失函数关于预测值的梯度：∂L/∂y_pred = 2/N * (y_pred - y)。
4. 反向传播：计算损失函数关于权重w和偏差b的梯度。
   ∂L/∂w = ∂L/∂y_pred * ∂y_pred/∂w = 2/N * (y_pred - y) * x
   ∂L/∂b = ∂L/∂y_pred * ∂y_pred/∂b = 2/N * (y_pred - y)
5. 更新权重和偏差：
   w = w - learning_rate * ∂L/∂w
   b = b - learning_rate * ∂L/∂b

现在，假设我们的训练集中的目标值y是非常大的数，而输入特征x的值相对较小。这样，当计算梯度时，由于y_pred - y的差异较大，导致梯度∂L/∂w和∂L/∂b也变得非常大。

例如，假设y = 1000，x = 1，w = 0.1，b = 0，learning_rate = 0.01。进行一次迭代的计算过程如下：

1. 前向传播：y_pred = 0.1 * 1 + 0 = 0.1
2. 计算梯度：∂L/∂y_pred = 2/1 * (0.1 - 1000) = -1999.8
3. 反向传播：
   ∂L/∂w = -1999.8 * 1 = -1999.8
   ∂L/∂b = -1999.8
4. 更新权重和偏差：
   w = 0.1 - 0.01 * (-1999.8) = 20.998
   b = 0 - 0.01 * (-1999.8) = 19.998

在这个例子中，我们可以看到由于梯度值非常大，权重w和偏差b的更新幅度也非常大。如果我们继续迭代，梯度可能会继续增大，导致权重和偏差发散到无穷大的值，模型无法收敛。

梯度爆炸问题可以通过多种方法来解决，例如梯度裁剪（gradient clipping）和使用更小的学习率。这些方法有助于限制梯度的大小，使其保持在可控的范围内，从而稳定模型的训练过程。

# Attention_Is_All_You_Need 翻译版

https://www.yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html

# Softmax函数

**Softmax函数**是一种特殊的函数，它能够将一组实数转换为一个概率分布。具体来说，对于一组实数，softmax函数会将每个实数转换为一个非负数，而且这些非负数的总和为1，因此它们可以被看作是一组事件发生的概率。

假设我们有一组实数x1，x2，...，xn，对于每一个实数xi，其对应的softmax函数值是这样计算的：

```scss
softmax(xi) = e^xi / (e^x1 + e^x2 + ... + e^xn)
```

其中，e是自然对数的底数，大约等于2.71828。

在自注意力机制中，softmax函数是用来计算每个词对于其他词的注意力权重。具体来说，我们首先计算出每个词对于其他词的注意力得分，然后使用softmax函数将这些得分转换为权重，使得所有词的权重总和为1。

对于一个非常简单的例子，假设我们有三个词，它们的注意力得分分别为1，2，3，那么我们可以计算出它们的注意力权重如下：

```scss
softmax(1) = e^1 / (e^1 + e^2 + e^3) ≈ 0.09
softmax(2) = e^2 / (e^1 + e^2 + e^3) ≈ 0.24
softmax(3) = e^3 / (e^1 + e^2 + e^3) ≈ 0.67
```

我们可以看到，这三个词的注意力权重都是非负数，而且它们的总和为1。同时，得分较高的词有更大的权重，这意味着我们的模型会更关注它。

# 全连接层的权重W 和 偏置b
对于神经网络和深度学习模型，权重（W）和偏置（b）是模型需要学习的参数。在开始训练时，这些参数通常会被初始化为小的随机数，这样可以打破模型的对称性，使得每个神经元在初始阶段学习到不同的特征。

全连接层（也被称为稠密层）的权重W和偏置b的初始值通常都是随机的。它们的维度由输入和输出的大小决定。例如，如果全连接层的输入大小为n，输出大小为m，那么权重W的大小就是[n, m]，偏置b的大小就是[m]。

在模型训练过程中，权重和偏置将通过反向传播和优化算法（如随机梯度下降）进行更新，以最小化模型的损失函数。

# attention模型
假设我们有一个双向的GRU模型作为编码器，我们要翻译的句子是 "Tom chase Jerry"，每个单词都表示为一个4维向量。这里的“attention”实际上是一个机制，用于确定在生成输出的每个步骤中，输入的哪些部分是相关的。

**Step 1: Tokenization and Embedding**

首先，我们将句子分割成单词，然后将每个单词表示为一个4维向量。

假设我们有以下嵌入：

- Tom: [0.1, 0.2, 0.3, 0.4]
- chase: [0.5, 0.6, 0.7, 0.8]
- Jerry: [0.9, 1.0, 1.1, 1.2]

**Step 2: Encoder - Bi-directional GRU**

接下来，我们将每个嵌入的词向量传递给编码器（双向GRU）。每个GRU单元的参数由你来定义。假设我们的GRU单元在经过训练后，得出以下编码向量：

- Tom: [0.2, 0.4, 0.6, 0.8]
- chase: [0.7, 0.9, 1.1, 1.3]
- Jerry: [1.2, 1.5, 1.8, 2.1]

**Step 3: Attention Calculation**

然后，我们将计算注意力权重。这里，我们使用的是点积注意力，即我们将编码器的输出和解码器的隐藏状态做点积，然后进行softmax操作以获取权重。

假设在解码的某一步，我们有一个解码器的隐藏状态为[0.6, 0.8, 1.0, 1.2]。首先，我们将这个隐藏状态和每个编码器的输出做点积：

- Tom: [0.2, 0.4, 0.6, 0.8] dot [0.6, 0.8, 1.0, 1.2] = 2.0
- chase: [0.7, 0.9, 1.1, 1.3] dot [0.6, 0.8, 1.0, 1.2] = 4.6
- Jerry: [1.2, 1.5, 1.8, 2.1] dot [0.6, 0.8, 1.0, 1.2] = 8.8

然后，我们用softmax函数将这些分数转化为概率：

- Tom: exp(2.0) / (exp(2.0) + exp(4.6) + exp(8.8)) ≈ 0.002
- chase: exp(4.6) / (exp(2.0) + exp(4.6) + exp(8.8)) ≈ 0.048
- Jerry: exp(8.8) / (exp(2.0) + exp(4.6) + exp(8.8)) ≈ 0.950

这意味着在生成这一步的翻译时，模型将更多地关注“Jerry”。

**Step 4: Context Vector and Decoder**

最后，我们用上述的权重对编码器的输出进行加权求和，得到上下文向量。以Jerry为例，它的上下文向量就是：

- Context vector: 0.002 * [0.2, 0.4, 0.6, 0.8] + 0.048 * [0.7, 0.9, 1.1, 1.3] + 0.950 * [1.2, 1.5, 1.8, 2.1] ≈ [1.14, 1.43, 1.72, 2.01]

接着，这个上下文向量和解码器的当前隐藏状态一起被送入解码器，用于生成下一个输出词。

以上就是一个基础的注意力模型的工作过程。

# Soft-Attention
Soft-Attention是一种非常常见的注意力机制。它的基本思想是，给定一个输入序列，模型在每一步生成输出时，都会查看整个输入序列，并赋予每个输入元素一个权重，这个权重就是所谓的“注意力权重”。

在我们上面的例子中，就使用了一种形式的Soft-Attention。那么，我们继续用这个例子，通过一步步的详细操作来解释Soft-Attention的工作方式：

**Step 1: Encoder - Bi-directional GRU**

假设我们在翻译一个句子，首先将输入的单词通过一个双向GRU的编码器处理，得到每个单词的编码向量：

- Tom: [0.2, 0.4, 0.6, 0.8]
- chase: [0.7, 0.9, 1.1, 1.3]
- Jerry: [1.2, 1.5, 1.8, 2.1]

**Step 2: Decoder - Calculate Attention Weight**

然后，假设在解码的某一步，我们有一个解码器的隐藏状态为[0.6, 0.8, 1.0, 1.2]。我们需要计算该隐藏状态与每个编码向量的相似度。常见的相似度度量方法有点积和全连接层，这里我们用点积：

- Tom: [0.2, 0.4, 0.6, 0.8] dot [0.6, 0.8, 1.0, 1.2] = 2.0
- chase: [0.7, 0.9, 1.1, 1.3] dot [0.6, 0.8, 1.0, 1.2] = 4.6
- Jerry: [1.2, 1.5, 1.8, 2.1] dot [0.6, 0.8, 1.0, 1.2] = 8.8

这些相似度分数被送入softmax函数，得到每个单词的注意力权重：

- Tom: exp(2.0) / (exp(2.0) + exp(4.6) + exp(8.8)) ≈ 0.002
- chase: exp(4.6) / (exp(2.0) + exp(4.6) + exp(8.8)) ≈ 0.048
- Jerry: exp(8.8) / (exp(2.0) + exp(4.6) + exp(8.8)) ≈ 0.950

这就是所谓的Soft-Attention。每个单词都被赋予了一个权重，代表了在这一步解码时，模型应该“关注”的程度。

**Step 3: Context Vector**

最后，我们计算上下文向量，它是每个编码向量与其对应的注意力权重的加权和：

- Context vector: 0.002 * [0.2, 0.4, 0.6, 0.8] + 0.048 * [0.7, 0.9, 1.1, 1.3] + 0.950 * [1.2, 1.5, 1.8, 2.1] ≈ [1.14, 1.43, 1.72, 2.01]

然后，这个上下文向量和解码器的当前隐藏状态一起被送入解码器，用于生成下一个输出词。

这就是Soft-Attention的基本原理。它的主要优点是可以考虑到输入序列的全部信息，但由于需要计算所有输入元素的权重，因此计算量较大

# --
```scss
-------------------------0、初始状态---------------------------------

每个单词的初始嵌入（embedding）
"Tom" = [1, 0, 0]
"chase" = [0, 1, 0]
"Jerry" = [0, 0, 1]

自注意力机制的参数
WQ(Query)=[[1, 0, 0],
           [0, 1, 0],
           [0, 0, 1]]

WK(Key）=[[1, 0, 0],
          [0, 1, 0],
          [0, 0, 1]]

WV(Value)=[[1, 0, 0],
           [0, 1, 0],
           [0, 0, 1]]

-------------------------1、Query、Key和Value的计算--------------------

对于每个词，我们都要计算一个Query、一个Key和一个Value。
因为我们的权重矩阵是单位矩阵，所以计算结果仍然是每个词的原始嵌入。
例如，"Tom"的Query、Key和Value都是[1, 0, 0]。

-------------------------2、注意力得分的计算----------------------------

注意力得分是通过计算Query和Key的点积得到的。
例如，"Tom"对于"chase"的注意力得分就是"Tom"的Query和"chase"的Key的点积，即[1, 0, 0]·[0, 1, 0] = 0。
同理，可以计算出所有的注意力得分。
        Tom    chase   Jerry
Tom      1       0       0
chase    0       1       0
Jerry    0       0       1
-------------------------2、注意力权重的计算----------------------------

接下来，我们需要计算注意力权重，这是通过对注意力得分进行softmax运算得到的。
在我们的例子中，每个词对于自己的注意力得分是1，对于其他词是0，所以每个词的注意力权重都是1。

```